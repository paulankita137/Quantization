# Quantization
Quantization of Neural Networks for model compression using bit precision. Create a quantized version of deep neural net with bit precision and compute energy consumed by the quantized model compared to the baseline model of 64 bits.

The quantization of 2bits have been carried out using the qkeras library :

https://github.com/google/qkeras
